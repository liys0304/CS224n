\titledquestion{Machine Learning \& Neural Networks}[8] 
\begin{parts}

    
    \part[4] Adam Optimizer\newline
        Recall the standard Stochastic Gradient Descent update rule:
        \alns{
            	\btheta_{t+1} &\gets \btheta_t - \alpha \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)
        }
        where $t+1$ is the current timestep, $\btheta$ is a vector containing all of the model parameters, ($\btheta_t$ is the model parameter at time step $t$, and $\btheta_{t+1}$ is the model parameter at time step $t+1$), $J$ is the loss function, $\nabla_{\btheta} J_{\text{minibatch}}(\btheta)$ is the gradient of the loss function with respect to the parameters on a minibatch of data, and $\alpha$ is the learning rate.
        Adam Optimization\footnote{Kingma and Ba, 2015, \url{https://arxiv.org/pdf/1412.6980.pdf}} uses a more sophisticated update rule with two additional steps.\footnote{The actual Adam update uses a few additional tricks that are less important, but we won't worry about them here. If you want to learn more about it, you can take a look at: \url{http://cs231n.github.io/neural-networks-3/\#sgd}}
            
        \begin{subparts}

            \subpart[2]First, Adam uses a trick called {\it momentum} by keeping track of $\bm$, a rolling average of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1}
                }
                where $\beta_1$ is a hyperparameter between 0 and 1 (often set to  0.9). Briefly explain in 2--4 sentences (you don't need to prove mathematically, just give an intuition) how using $\bm$ stops the updates from varying as much and why this low variance may be helpful to learning, overall.\newline
              \begin{shaded}
              	Answer:$\bm $ keeps some previous value while updating the value by times a hyperparameter(which is small). The low variance could make the parameter still update towards the lowest direction without the problem of explosive gradient.
              \end{shaded}
                
            \subpart[2] Adam extends the idea of {\it momentum} with the trick of {\it adaptive learning rates} by keeping track of  $\bv$, a rolling average of the magnitudes of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\bv_{t+1} &\gets \beta_2\bv_{t} + (1 - \beta_2) (\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \odot \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1} / \sqrt{\bv_{t+1}}
                }
                where $\odot$ and $/$ denote elementwise multiplication and division (so $\bz \odot \bz$ is elementwise squaring) and $\beta_2$ is a hyperparameter between 0 and 1 (often set to  0.99). Since Adam divides the update by $\sqrt{\bv}$, which of the model parameters will get larger updates?  Why might this help with learning?
                
           \begin{shaded}
           Answer: In Adam algorithm, the parameters with smaller $\bv$ will get larger updates. 
           Smoothing the optimization path: Parameters with large gradients (typically indicating noisy or highly sensitive parameters) are updated more conservatively, which reduces the risk of oscillations or divergence.
           Balancing learning rates: Parameters with small gradients (often associated with "slow-moving" or "flat" regions in the loss landscape) are given larger updates, which helps escape from shallow or flat areas.
           Automatic adjustment: By dynamically adapting the learning rate per parameter, Adam can effectively manage a wider range of gradients and automatically adjust for different scales in different directions, speeding up convergence without extensive manual tuning of the learning rate.
           \end{shaded}
                
                \end{subparts}
        
        
            \part[4] 
            Dropout\footnote{Srivastava et al., 2014, \url{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}} is a regularization technique. During training, dropout randomly sets units in the hidden layer $\bh$ to zero with probability $p_{\text{drop}}$ (dropping different units each minibatch), and then multiplies $\bh$ by a constant $\gamma$. We can write this as:
                \alns{
                	\bh_{\text{drop}} = \gamma \bd \odot \bh
                }
                where $\bd \in \{0, 1\}^{D_h}$ ($D_h$ is the size of $\bh$)
                is a mask vector where each entry is 0 with probability $p_{\text{drop}}$ and 1 with probability $(1 - p_{\text{drop}})$. $\gamma$ is chosen such that the expected value of $\bh_{\text{drop}}$ is $\bh$:
                \alns{
                	\mathbb{E}_{p_{\text{drop}}}[\bh_\text{drop}]_i = h_i \text{\phantom{aaaa}}
                }
                for all $i \in \{1,\dots,D_h\}$. 
            \begin{subparts}
            \subpart[2]
                What must $\gamma$ equal in terms of $p_{\text{drop}}$? Briefly justify your answer or show your math derivation using the equations given above.
                
            \begin{shaded}
            	We want $\mathbb{E}_{p_{\text{drop}}}[\bh_\text{drop}]_i = h_i$, \\
            	$\bh_{\text{drop}} = \gamma \bd \odot \bh$ \\
            	$\mathbb{E}_{p_{\text{drop}}}[\bh_\text{drop}]_i = \gamma h_i \mathbb{E}[d_i] = \gamma h_i(1-p_{drop})$
            	a.k.a $\gamma h_i(1-p_{drop}) = h_i$ \\
            	$\gamma = \frac{1}{1-p_{\text{drop}}}$
            \end{shaded}
          \subpart[2] Why should dropout be applied during training? Why should dropout \textbf{NOT} be applied during evaluation? (Hint: it may help to look at the paper linked above in the write-up.) \newline
          \begin{shaded}
          During training, dropout is applied to prevent overfitting by randomly setting units in the hidden layer to zero, which forces the network to learn more robust representations. This technique encourages redundancy and reduces reliance on specific neurons, improving generalization to new data.
          During evaluation, however, dropout is not applied because we want to use the full capacity of the network for predictions. At this stage, we simply scale the output by the factor $\gamma = \frac{1}{1-p_{\text{drop}}}$
          (which was learned during training) to account for the average scaling effect of dropout. Applying dropout during evaluation would introduce unnecessary randomness, reducing the model's predictive accuracy and consistency.
          \end{shaded}
         
        \end{subparts}


\end{parts}
